# -*- coding: utf-8 -*-
# Copyright 2007-2022 The HyperSpy developers
#
# This file is part of RosettaSciIO.
#
# RosettaSciIO is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# RosettaSciIO is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with RosettaSciIO. If not, see <https://www.gnu.org/licenses/#GPL>.

# The EMD format is a hdf5 standard proposed at Lawrence Berkeley
# National Lab (see https://emdatasets.com/ for more information).
# FEI later developed another EMD format, also based on the hdf5 standard. This
# reader first checked if the file have been saved by Velox (FEI EMD format)
# and use either the EMD class or the FEIEMDReader class to read the file.
# Writing file is only supported for EMD Berkeley file.


import re
import json
import os
from datetime import datetime
import time
import warnings
import math
import logging

import h5py
import numpy as np
import dask.array as da
from dateutil import tz

from rsciio.utils.tools import _UREG, DTBox
from rsciio.utils.elements import atomic_number2name
import rsciio.utils.fei_stream_readers as stream_readers
from rsciio._hierarchical import get_signal_chunks


EMD_VERSION = "0.2"

_logger = logging.getLogger(__name__)


class EMD_NCEM:

    """Class for reading and writing the Berkeley variant of the electron
    microscopy datasets (EMD) file format. It reads files EMD NCEM, including
    files generated by the prismatic software.

    Attributes
    ----------
    dictionaries: list
        List of dictionaries which are passed to the file_reader.
    """

    def read_file(self, file, lazy=None, dataset_path=None, stack_group=None):
        """
        Read the data from an emd file

        Parameters
        ----------
        file : file handle
            Handle of the file to read the data from.
        lazy : bool, optional
            Load the data lazily. The default is False.
        dataset_path : None, str or list of str
            Path of the dataset. If None, load all supported datasets,
            otherwise the specified dataset. The default is None.
        stack_group : bool, optional
            Stack datasets of groups with common name. Relevant for emd file
            version >= 0.5 where groups can be named 'group0000', 'group0001',
            etc.
        """
        self.file = file
        self.lazy = lazy

        if isinstance(dataset_path, list):
            if stack_group:
                _logger.warning(
                    "The argument 'dataset_path' and "
                    "'stack_group' are not compatible."
                )
            stack_group = False
            dataset_path = dataset_path.copy()
        elif isinstance(dataset_path, str):
            dataset_path = [dataset_path]
        # if 'datasets' is not provided, we load all valid datasets
        elif dataset_path is None:
            dataset_path = self.find_dataset_paths(file)
            if stack_group is None:
                stack_group = True

        self.dictionaries = []

        while len(dataset_path) > 0:
            path = dataset_path.pop(0)
            group_paths = [os.path.dirname(path)]
            dataset_name = os.path.basename(path)

            if stack_group:
                # Find all the datasets in this group which are also listed
                # in dataset_path:
                # 1. add them to 'group_paths'
                # 2. remove them from 'dataset_path'
                group_basename = group_paths[0]
                if self._is_prismatic_file and "ppotential" not in path:
                    # In prismatic file, the group name have '0000' except
                    # for 'ppotential'
                    group_basename = group_basename[:-4]
                for _path in dataset_path[:]:
                    if path != _path and group_basename in _path:
                        group_paths.append(os.path.dirname(_path))
                        dataset_path.remove(_path)
                title = os.path.basename(group_basename)
            else:
                title = os.path.basename(group_paths[0])

            _logger.debug(f"Loading dataset: {path}")

            om = self._parse_original_metadata()
            data, axes = self._read_data_from_groups(
                group_paths, dataset_name, title, om
            )

            md = self._parse_metadata(group_paths[0], title=title)
            d = {
                "data": data,
                "axes": axes,
                "metadata": md,
                "original_metadata": om,
            }
            self.dictionaries.append(d)

    @classmethod
    def find_dataset_paths(cls, file, supported_dataset=True):
        """
        Find the paths of all groups containing valid EMD data.

        Parameters
        ----------
        file : hdf5 file handle
        supported_dataset : bool, optional
            If True (default), returns the paths of all supported datasets,
            otherwise returns the path of the non-supported other dataset.
            This is relevant for groups containing auxiliary dataset(s) which
            are not supported by HyperSpy or described in the EMD NCEM dataset
            specification.

        Returns
        -------
        datasets : list
            List of path to these group.

        """

        def print_dataset_only(item_name, item, dataset_only):
            if supported_dataset is os.path.basename(item_name).startswith(
                (
                    "data",
                    "counted_datacube",
                    "datacube",
                    "diffractionslice",
                    "realslice",
                    "pointlistarray",
                    "pointlist",
                )
            ):
                if isinstance(item, h5py.Dataset):
                    grp = file.get(os.path.dirname(item_name))
                    if cls._get_emd_group_type(grp):
                        dataset_path.append(item_name)

        f = lambda item_name, item: print_dataset_only(
            item_name, item, supported_dataset
        )

        dataset_path = []
        file.visititems(f)

        return dataset_path

    @property
    def _is_prismatic_file(self):
        return True if "4DSTEM_simulation" in self.file.keys() else False

    @property
    def _is_py4DSTEM_file(self):
        return True if "4DSTEM_experiment" in self.file.keys() else False

    @staticmethod
    def _get_emd_group_type(group):
        """Return the value of the 'emd_group_type' attribute if it exist,
        otherwise returns False
        """
        return group.attrs.get("emd_group_type", False)

    @staticmethod
    def _read_dataset(dataset):
        """Read dataset and use the h5py AsStrWrapper when the dataset is of
        string type (h5py 3.0 and newer)
        """
        chunks = dataset.chunks
        if chunks is None:
            chunks = "auto"
        if h5py.check_string_dtype(dataset.dtype) and hasattr(dataset, "asstr"):
            # h5py 3.0 and newer
            # https://docs.h5py.org/en/3.0.0/strings.html
            dataset = dataset.asstr()[:]
        return dataset, chunks

    def _read_emd_version(self, group):
        """Return the group version if the group is an EMD group, otherwise
        return None.
        """
        if "version_major" in group.attrs.keys():
            version = [
                str(group.attrs.get(v)) for v in ["version_major", "version_minor"]
            ]
            version = ".".join(version)
            return version

    def _read_data_from_groups(
        self, group_path, dataset_name, stack_key=None, original_metadata={}
    ):
        axes = []
        transpose_required = True if dataset_name != "datacube" else False

        array_list = [self.file.get(f"{key}/{dataset_name}") for key in group_path]

        if None in array_list:
            raise IOError("Dataset can't be found.")

        if len(array_list) > 1:
            # Squeeze the data only when
            if self.lazy:
                data_list = [da.from_array(*self._read_dataset(d)) for d in array_list]
                if transpose_required:
                    data_list = [da.transpose(d) for d in data_list]
                data = da.stack(data_list)
                data = da.squeeze(data)
            else:
                data_list = [
                    np.asanyarray(self._read_dataset(d)[0]) for d in array_list
                ]
                if transpose_required:
                    data_list = [np.transpose(d) for d in data_list]
                data = np.stack(data_list).squeeze()
        else:
            d = array_list[0]
            if self.lazy:
                data = da.from_array(*self._read_dataset(d))
            else:
                data = np.asanyarray(self._read_dataset(d)[0])
            if transpose_required:
                data = data.transpose()

        shape = data.shape

        if len(array_list) > 1:
            offset, scale, units = 0, 1, None
            if self._is_prismatic_file and "depth" in stack_key:
                simu_om = original_metadata.get("simulation_parameters", {})
                if "numSlices" in simu_om.keys():
                    scale = simu_om["numSlices"]
                    scale *= simu_om.get("sliceThickness", 1.0)
                if "zStart" in simu_om.keys():
                    offset = simu_om["zStart"]
                    # when zStart = 0, the first image is not at zero but
                    # the first output: numSlices * sliceThickness (=scale)
                    if offset == 0:
                        offset = scale
                units = "Å"
                total_thickness = (
                    simu_om.get("tile", 0)[2] * simu_om.get("cellDimension", 0)[0]
                )
                if not math.isclose(
                    total_thickness, len(array_list) * scale, rel_tol=1e-4
                ):
                    _logger.warning(
                        "Depth axis is non-uniform and its offset "
                        "and scale can't be set accurately."
                    )
                    # When non-uniform/non-linear axis are implemented, adjust
                    # the final depth to the "total_thickness"
                    offset, scale, units = 0, 1, None
            axes.append(
                {
                    "index_in_array": 0,
                    "name": stack_key if stack_key is not None else None,
                    "offset": offset,
                    "scale": scale,
                    "size": len(array_list),
                    "units": units,
                    "navigate": True,
                }
            )

            array_indices = np.arange(1, len(shape))
            dim_indices = array_indices
        else:
            array_indices = np.arange(0, len(shape))
            # dim indices start form 1
            dim_indices = array_indices + 1

        if transpose_required:
            dim_indices = dim_indices[::-1]

        for arr_index, dim_index in zip(array_indices, dim_indices):
            dim = self.file.get(f"{group_path[0]}/dim{dim_index}")
            offset, scale = self._parse_axis(dim)
            if self._is_prismatic_file:
                if dataset_name == "datacube":
                    # For datacube (4D STEM), the signal is detector coordinate
                    sig_dim = ["dim3", "dim4"]
                else:
                    sig_dim = ["dim1", "dim2"]

                navigate = dim.name.split("/")[-1] not in sig_dim

            else:
                navigate = False
            axes.append(
                {
                    "index_in_array": arr_index,
                    "name": self._parse_attribute(dim, "name"),
                    "units": self._parse_attribute(dim, "units"),
                    "size": shape[arr_index],
                    "offset": offset,
                    "scale": scale,
                    "navigate": navigate,
                }
            )
        return data, axes

    def _parse_attribute(self, obj, key):
        value = obj.attrs.get(key)
        if value is not None:
            if not isinstance(value, str):
                value = value.decode()
            if key == "units":
                # Get all the units
                units_list = re.findall(r"(\[.+?\])", value)
                units_list = [u[1:-1].replace("_", "") for u in units_list]
                value = " * ".join(units_list)
                try:
                    units = _UREG.parse_units(value)
                    value = f"{units:~}"
                except Exception:
                    pass
        return value

    def _parse_metadata(self, group_basename, title=""):
        filename = self.file if isinstance(self.file, str) else self.file.filename
        md = {
            "General": {
                "title": title.replace("_depth", ""),
                "original_filename": os.path.split(filename)[1],
            },
            "Signal": {"signal_type": ""},
        }
        if "CBED" in group_basename:
            md["Signal"]["signal_type"] = "electron_diffraction"
        return md

    def _parse_original_metadata(self):
        f = self.file
        om = {"EMD_version": self._read_emd_version(self.file.get("/"))}
        for group_name in ["microscope", "sample", "user", "comments"]:
            group = f.get(group_name)
            if group is not None:
                om.update(
                    {group_name: {key: value for key, value in group.attrs.items()}}
                )

        if self._is_prismatic_file:
            md_mapping = {
                "i": "filenameAtoms",
                "a": "algorithm",
                "fx": "interpolationFactorX",
                "fy": "interpolationFactorY",
                "F": "numFP",
                "ns": "numSlices",
                "te": "includeThermalEffects",
                "oc": "includeOccupancy",
                "3D": "save3DOutput",
                "4D": "save3DOutput",
                "DPC": "saveDPC_CoM",
                "ps": "savePotentialSlices",
                "nqs": "nyquistSampling",
                "px": "realspacePixelSizeX",
                "py": "realspacePixelSizeY",
                "P": "potBound",
                "s": "sliceThickness",
                "zs": "zStart",
                "E": "E0",
                "A": "alphaBeamMax",
                "rx": "probeStepX",
                "ry": "probeStepY",
                "df": "probeDefocus",
                "sa": "probeSemiangle",
                "d": "detectorAngleStep",
                "tx": "probeXtilt",
                "ty": "probeYtilt",
                "c": "cellDimension",
                "t": "tile",
                "wx": "scanWindowX",
                "wy": "scanWindowY",
                "wxr": "scanWindowX_r",
                "wyr": "scanWindowY_r",
                "2D": "integrationAngle",
            }
            simu_md = f.get(
                "4DSTEM_simulation/metadata/metadata_0/original/simulation_parameters"
            )
            om["simulation_parameters"] = {
                md_mapping.get(k, k): v for k, v in simu_md.attrs.items()
            }

        return om

    @staticmethod
    def _parse_axis(axis_data):
        """
        Estimate, offset, scale from a 1D array
        """
        if axis_data.ndim > 0 and np.issubdtype(axis_data.dtype, np.number):
            offset, scale = axis_data[0], np.diff(axis_data).mean()
        else:
            # This is a string, return default values
            # When non-uniform axis is supported we should be able to parse
            # string
            offset, scale = 0, 1
        return offset, scale

    def write_file(self, file, signal, **kwargs):
        """
        Write signal to file.

        Parameters
        ----------
        file : str of h5py file handle
            If str, filename of the file to write, otherwise a h5py file handle
        signal : instance of hyperspy signal
            The signal to save.
        **kwargs : dict
            Keyword argument are passed to the ``h5py.Group.create_dataset``
            method.

        """
        if isinstance(file, str):
            emd_file = h5py.File(file, "w")
        # Write version:
        ver_maj, ver_min = EMD_VERSION.split(".")
        emd_file.attrs["version_major"] = ver_maj
        emd_file.attrs["version_minor"] = ver_min

        # Write attribute from the original_metadata
        om = DTBox(signal["original_metadata"], box_dots=True)
        for group_name in ["microscope", "sample", "user", "comments"]:
            group = emd_file.require_group(group_name)
            d = om.get(group_name, None)
            if d is not None:
                for key, value in d.items():
                    group.attrs[key] = value

        # Write signals:
        signal_group = emd_file.require_group("signals")
        signal_group.attrs["emd_group_type"] = 1
        self._write_signal_to_group(signal_group, signal, **kwargs)
        emd_file.close()

    def _write_signal_to_group(self, signal_group, signal, chunks=None, **kwargs):
        # Save data:
        title = signal["metadata"]["General"]["title"] or "__unnamed__"
        dataset = signal_group.require_group(title)
        data = signal["data"].T
        maxshape = tuple(None for _ in data.shape)
        if np.issubdtype(data.dtype, np.dtype("U")):
            # Saving numpy unicode type is not supported in h5py
            data = data.astype(np.dtype("S"))
        if chunks is None:
            if isinstance(data, da.Array):
                # For lazy dataset, by default, we use the current dask chunking
                chunks = tuple([c[0] for c in data.chunks])
            else:
                signal_axes = [
                    i for i, axis in enumerate(signal["axes"]) if not axis["navigate"]
                ]
                chunks = get_signal_chunks(data.shape, data.dtype, signal_axes)
        # when chunks=True, we leave it to h5py `guess_chunk`
        elif chunks is not True:
            # Need to reverse since the data is transposed when saving
            chunks = chunks[::-1]

        dataset.create_dataset(
            "data", data=data, maxshape=maxshape, chunks=chunks, **kwargs
        )

        array_indices = np.arange(0, len(data.shape))
        dim_indices = (array_indices + 1)[::-1]
        # Iterate over all dimensions:
        for i, dim_index in zip(array_indices, dim_indices):
            key = f"dim{dim_index}"
            axis = signal["axes"][i]
            offset = axis["offset"]
            scale = axis["scale"]
            dim = dataset.create_dataset(key, data=[offset, offset + scale])
            name = axis["name"]
            if name is None:
                name = ""
            dim.attrs["name"] = name
            units = axis["units"]
            if units is None:
                units = ""
            else:
                units = "[{}]".format("_".join(list(units)))
            dim.attrs["units"] = units
        # Write metadata:
        dataset.attrs["emd_group_type"] = 1
        for key, value in signal["metadata"]["Signal"].items():
            try:  # If something h5py can't handle is saved in the metadata...
                dataset.attrs[key] = value
            except Exception:  # ...let the user know what could not be added!
                _logger.warning(
                    "The following information couldn't be "
                    f"written in the file: {key}: {value}"
                )


def _get_keys_from_group(group):
    # Return a list of ids of items contains in the group
    return list(group.keys())


def _parse_sub_data_group_metadata(sub_data_group):
    metadata_array = sub_data_group["Metadata"][:, 0].T
    mdata_string = metadata_array.tobytes().decode("utf-8")
    return json.loads(mdata_string.rstrip("\x00"))


def _parse_metadata(data_group, sub_group_key):
    return _parse_sub_data_group_metadata(data_group[sub_group_key])


def _get_detector_metadata_dict(om, detector_name):
    detectors_dict = om["Detectors"]
    # find detector dict from the detector_name
    for key in detectors_dict:
        if detectors_dict[key]["DetectorName"] == detector_name:
            return detectors_dict[key]


class FeiEMDReader(object):
    """
    Class for reading FEI electron microscopy datasets.

    The :class:`~.FeiEMDReader` reads EMD files saved by the FEI Velox
    software package.

    Attributes
    ----------
    dictionaries: list
        List of dictionaries which are passed to the file_reader.
    im_type : string
        String specifying whether the data is an image, spectrum or
        spectrum image.

    """

    def __init__(
        self,
        filename=None,
        select_type=None,
        first_frame=0,
        last_frame=None,
        sum_frames=True,
        sum_EDS_detectors=True,
        rebin_energy=1,
        SI_dtype=None,
        load_SI_image_stack=False,
        lazy=False,
    ):
        # TODO: Finish lazy implementation using the `FrameLocationTable`
        # Parallelise streams reading
        self.filename = filename
        self.select_type = select_type
        self.dictionaries = []
        self.first_frame = first_frame
        self.last_frame = last_frame
        self.sum_frames = sum_frames
        self.sum_EDS_detectors = sum_EDS_detectors
        self.rebin_energy = rebin_energy
        self.SI_data_dtype = SI_dtype
        self.load_SI_image_stack = load_SI_image_stack
        self.lazy = lazy
        self.detector_name = None
        self.original_metadata = {}

    def read_file(self, f):
        self.filename = f.filename
        self.d_grp = f.get("Data")
        self._check_im_type()
        self._parse_metadata_group(f.get("Operations"), "Operations")
        if self.im_type == "SpectrumStream":
            self.p_grp = f.get("Presentation")
            self._parse_image_display()
        self._read_data(self.select_type)

    def _read_data(self, select_type):
        self.load_images = self.load_SI = self.load_single_spectrum = True
        if select_type == "single_spectrum":
            self.load_images = self.load_SI = False
        elif select_type == "images":
            self.load_SI = self.load_single_spectrum = False
        elif select_type == "spectrum_image":
            self.load_images = self.load_single_spectrum = False
        elif select_type is None:
            pass
        else:
            raise ValueError(
                "`select_type` parameter takes only: `None`, "
                "'single_spectrum', 'images' or 'spectrum_image'."
            )

        if self.im_type == "Image":
            _logger.info("Reading the images.")
            self._read_images()
        elif self.im_type == "Spectrum":
            self._read_single_spectrum()
            self._read_images()
        elif self.im_type == "SpectrumStream":
            self._read_single_spectrum()
            _logger.info("Reading the spectrum image.")
            t0 = time.time()
            self._read_images()
            t1 = time.time()
            self._read_spectrum_stream()
            t2 = time.time()
            _logger.info("Time to load images: {} s.".format(t1 - t0))
            _logger.info("Time to load spectrum image: {} s.".format(t2 - t1))

    def _check_im_type(self):
        if "Image" in self.d_grp:
            if "SpectrumImage" in self.d_grp:
                self.im_type = "SpectrumStream"
            else:
                self.im_type = "Image"
        else:
            self.im_type = "Spectrum"

    def _read_single_spectrum(self):
        if not self.load_single_spectrum:
            return
        spectrum_grp = self.d_grp.get("Spectrum")
        if spectrum_grp is None:
            return  # No spectra in the file
        self.detector_name = "EDS"
        for spectrum_sub_group_key in _get_keys_from_group(spectrum_grp):
            self.dictionaries.append(
                self._read_spectrum(spectrum_grp, spectrum_sub_group_key)
            )

    def _read_spectrum(self, spectrum_group, spectrum_sub_group_key):
        spectrum_sub_group = spectrum_group[spectrum_sub_group_key]
        dataset = spectrum_sub_group["Data"]
        if self.lazy:
            data = da.from_array(dataset, chunks=dataset.chunks).T
        else:
            data = dataset[:].T
        original_metadata = _parse_metadata(spectrum_group, spectrum_sub_group_key)
        original_metadata.update(self.original_metadata)

        # Can be used in more recent version of velox emd files
        self.detector_information = self._get_detector_information(original_metadata)

        dispersion, offset, unit = self._get_dispersion_offset(original_metadata)
        axes = []
        if len(data.shape) == 2:
            if data.shape[0] == 1:
                # squeeze
                data = data[0, :]
            else:
                axes = [
                    {
                        "name": "Stack",
                        "offset": 0,
                        "scale": 1,
                        "size": data.shape[0],
                        "navigate": True,
                    }
                ]
        axes.append(
            {
                "name": "Energy",
                "offset": offset,
                "scale": dispersion,
                "size": data.shape[-1],
                "units": "keV",
                "navigate": False,
            },
        )

        md = self._get_metadata_dict(original_metadata)
        md["Signal"]["signal_type"] = "EDS_TEM"

        return {
            "data": data,
            "axes": axes,
            "metadata": md,
            "original_metadata": original_metadata,
            "mapping": self._get_mapping(),
        }

    def _read_images(self):
        # We need to read the image to get the shape of the spectrum image
        if not self.load_images and not self.load_SI:
            return
        # Get the image data group
        image_group = self.d_grp.get("Image")
        if image_group is None:
            return  # No images in the file
        # Get all the subgroup of the image data group and read the image for
        # each of them
        for image_sub_group_key in _get_keys_from_group(image_group):
            image = self._read_image(image_group, image_sub_group_key)
            if not self.load_images:
                # If we don't want to load the images, we stop here
                return
            self.dictionaries.append(image)

    def _read_image(self, image_group, image_sub_group_key):
        """Return a dictionary ready to parse of return to io module"""
        image_sub_group = image_group[image_sub_group_key]
        original_metadata = _parse_metadata(image_group, image_sub_group_key)
        original_metadata.update(self.original_metadata)

        # Can be used in more recent version of velox emd files
        self.detector_information = self._get_detector_information(original_metadata)
        self.detector_name = self._get_detector_name(image_sub_group_key)

        read_stack = self.load_SI_image_stack or self.im_type == "Image"
        h5data = image_sub_group["Data"]
        # Get the scanning area shape of the SI from the images
        self.spatial_shape = h5data.shape[:-1]
        # For Velox FFT data, dtype must be specified and lazy is not
        # supported due to special dtype. The data is loaded as-is; to get
        # a traditional view the negative half must be created and the data
        # must be re-centered
        # Similar story for DPC signal
        fft_dtype = [
            [("realFloatHalfEven", "<f4"), ("imagFloatHalfEven", "<f4")],
            [("realFloatHalfOdd", "<f4"), ("imagFloatHalfOdd", "<f4")],
        ]
        dpc_dtype = [("realFloat", "<f4"), ("imagFloat", "<f4")]
        if h5data.dtype in fft_dtype or h5data.dtype == dpc_dtype:
            _logger.debug("Found an FFT or DPC, loading as Complex2DSignal")
            real = h5data.dtype.descr[0][0]
            imag = h5data.dtype.descr[1][0]
            if self.lazy:
                data = da.from_array(h5data, chunks=h5data.chunks)
                data = data[real] + 1j * data[imag]
                data = da.transpose(data, axes=[2, 0, 1])
            else:
                data = np.empty(h5data.shape, h5data.dtype)
                h5data.read_direct(data)
                data = data[real] + 1j * data[imag]
                # Set the axes in frame, y, x order
                data = np.rollaxis(data, axis=2)
        else:
            if self.lazy:
                data = da.transpose(
                    da.from_array(h5data, chunks=h5data.chunks), axes=[2, 0, 1]
                )
            else:
                # Workaround for a h5py bug https://github.com/h5py/h5py/issues/977
                # Change back to standard API once issue #977 is fixed.
                # Preallocate the numpy array and use read_direct method, which is
                # much faster in case of chunked data.
                # Do not specify dtype in np.empty, slows down substantially!
                data = np.empty(h5data.shape)
                h5data.read_direct(data)
                # Set the axes in frame, y, x order
                data = np.rollaxis(data, axis=2)

        pix_scale = original_metadata["BinaryResult"].get(
            "PixelSize", {"height": 1.0, "width": 1.0}
        )
        offsets = original_metadata["BinaryResult"].get("Offset", {"x": 0.0, "y": 0.0})
        original_units = original_metadata["BinaryResult"].get("PixelUnitX", "")

        axes = []
        # stack of images
        if not read_stack:
            data = data[0:1, ...]

        if data.shape[0] == 1:
            # Squeeze
            data = data[0, ...]
            i = 0
        else:
            if "FrameTime" in original_metadata["Scan"]:
                frame_time = original_metadata["Scan"]["FrameTime"]
            else:
                _logger.debug("No Frametime found, likely TEM image stack")
                det_ind = original_metadata["BinaryResult"]["DetectorIndex"]
                frame_time = original_metadata["Detectors"][f"Detector-{det_ind}"][
                    "ExposureTime"
                ]
            frame_time, time_unit = self._convert_scale_units(
                frame_time, "s", 2 * data.shape[0]
            )
            axes.append(
                {
                    "index_in_array": 0,
                    "name": "Time",
                    "offset": 0,
                    "scale": frame_time,
                    "size": data.shape[0],
                    "units": time_unit,
                    "navigate": True,
                }
            )
            i = 1
        scale_x = self._convert_scale_units(
            pix_scale["width"], original_units, data.shape[i + 1]
        )
        scale_y = self._convert_scale_units(
            pix_scale["height"], original_units, data.shape[i]
        )
        offset_x = self._convert_scale_units(
            offsets["x"], original_units, data.shape[i + 1]
        )
        offset_y = self._convert_scale_units(
            offsets["y"], original_units, data.shape[i]
        )
        axes.extend(
            [
                {
                    "index_in_array": i,
                    "name": "y",
                    "offset": offset_y[0],
                    "scale": scale_y[0],
                    "size": data.shape[i],
                    "units": scale_y[1],
                    "navigate": False,
                },
                {
                    "index_in_array": i + 1,
                    "name": "x",
                    "offset": offset_x[0],
                    "scale": scale_x[0],
                    "size": data.shape[i + 1],
                    "units": scale_x[1],
                    "navigate": False,
                },
            ]
        )

        md = self._get_metadata_dict(original_metadata)
        if self.detector_name is not None:
            original_metadata["DetectorMetadata"] = _get_detector_metadata_dict(
                original_metadata, self.detector_name
            )
        if hasattr(self, "map_label_dict"):
            if image_sub_group_key in self.map_label_dict:
                md["General"]["title"] = self.map_label_dict[image_sub_group_key]

        return {
            "data": data,
            "axes": axes,
            "metadata": md,
            "original_metadata": original_metadata,
            "mapping": self._get_mapping(
                map_selected_element=False, parse_individual_EDS_detector_metadata=False
            ),
        }

    def _get_detector_name(self, key):
        def iDPC_or_dDPC(metadata):
            return "iDPC" if metadata == "true" else "dDPC"

        om = self.original_metadata["Operations"]
        keys = [
            "CameraInputOperation",
            "StemInputOperation",
            "SurfaceReconstructionOperation",
            "MathematicsOperation",
            "DpcOperation",
            "IntegrationOperation",
            "FftOperation",
        ]

        for k in keys:
            if k in om.keys() and k == keys[0]:
                for metadata in om[k].items():
                    # Find the metadata group matching the key in the dataPath
                    if key in metadata[1]["dataPath"]:
                        return metadata[1]["cameraName"]
            if k in om.keys() and k == keys[1]:
                for metadata in om[k].items():
                    # Find the metadata group matching the key in the dataPath
                    if key in metadata[1]["dataPath"]:
                        return metadata[1]["detector"]
            if k in om.keys() and k == keys[2]:
                for metadata in om[k].items():
                    # Look first for the key in the unfilteredDataPath
                    if "unfilteredDataPath" in metadata[1].keys() and (
                        key in metadata[1]["unfilteredDataPath"]
                    ):
                        return iDPC_or_dDPC(metadata[1]["integrationMode"])
                    # Then look for the key in the DataPath
                    if key in metadata[1]["dataPath"]:
                        detector_name = iDPC_or_dDPC(metadata[1]["integrationMode"])
                        if metadata[1]["enableFilter"] == "true":
                            detector_name = "Filtered {}".format(detector_name)
                        return detector_name
            if k in om.keys() and k == keys[3]:
                for metadata in om[k].items():
                    if key in metadata[1]["dataPath"]:
                        if metadata[1]["outputs"][0]["inputIndex"] == "0":
                            return "A-C"
                        elif metadata[1]["outputs"][0]["inputIndex"] == "1":
                            return "B-D"
            if k in om.keys() and k == keys[4]:
                for metadata in om[k].items():
                    if key in metadata[1]["dataPath"]:
                        return "DPC"
            if k in om.keys() and k == keys[5]:
                for metadata in om[k].items():
                    if key in metadata[1]["dataPath"]:
                        return "DCFI"
            if k in om.keys() and k == keys[6]:
                for metadata in om[k].items():
                    if key in metadata[1]["imageOutputPath"]:
                        return "Half FFT"
        return "Unrecognized_image_signal"

    def _get_detector_information(self, om):
        # if the `BinaryResult/Detector` is not available, there should be only
        # one detector in `Detectors`:
        # e.g. original_metadata['Detectors']['Detector-0']
        if "BinaryResult" in om.keys():
            detector_index = om["BinaryResult"].get("DetectorIndex")
        else:
            detector_index = 0
        if detector_index is not None:
            return om["Detectors"]["Detector-{}".format(detector_index)]

    def _parse_frame_time(self, original_metadata, factor=1):
        try:
            frame_time = original_metadata["Scan"]["FrameTime"]
            time_unit = "s"
        except KeyError:
            frame_time, time_unit = None, None

        frame_time, time_unit = self._convert_scale_units(frame_time, time_unit, factor)
        return frame_time, time_unit

    def _parse_image_display(self):
        try:
            image_display_group = self.p_grp.get("Displays/ImageDisplay")
            key_list = _get_keys_from_group(image_display_group)
            self.map_label_dict = {}
            for key in key_list:
                v = json.loads(image_display_group[key][0].decode("utf-8"))
                data_key = v["dataPath"].split("/")[-1]  # key in data group
                self.map_label_dict[data_key] = v["display"]["label"]
        except KeyError:
            _logger.warning("The image label can't be read from the metadata.")

    def _parse_metadata_group(self, group, group_name):
        d = {}
        try:
            for group_key in _get_keys_from_group(group):
                subgroup = group.get(group_key)
                if hasattr(subgroup, "keys"):
                    sub_dict = {}
                    for subgroup_key in _get_keys_from_group(subgroup):
                        v = json.loads(subgroup[subgroup_key][0].decode("utf-8"))
                        sub_dict[subgroup_key] = v
                else:
                    sub_dict = json.loads(subgroup[0].decode("utf-8"))
                d[group_key] = sub_dict
        except IndexError:
            _logger.warning("Some metadata can't be read.")
        self.original_metadata.update({group_name: d})

    def _read_spectrum_stream(self):
        if not self.load_SI:
            return
        self.detector_name = "EDS"
        # Try to read the number of frames from Data/SpectrumImage
        try:
            sig = self.d_grp["SpectrumImage"]
            self.number_of_frames = int(
                json.loads(
                    sig[next(iter(sig))]["SpectrumImageSettings"][0].decode("utf8")
                )["endFramePosition"]
            )
        except Exception:
            _logger.exception(
                "Failed to read the number of frames from Data/SpectrumImage"
            )
            self.number_of_frames = None
        if self.last_frame is None:
            self.last_frame = self.number_of_frames
        elif self.number_of_frames and self.last_frame > self.number_of_frames:
            raise ValueError(
                "The `last_frame` cannot be greater than"
                " the number of frames, %i for this file." % self.number_of_frames
            )

        spectrum_stream_group = self.d_grp.get("SpectrumStream")
        if spectrum_stream_group is None:
            _logger.warning(
                "No spectrum stream is present in the file. It "
                "is possible that the file has been pruned: use "
                "Velox to read the spectrum image (proprietary "
                "format). If you want to open FEI emd file with "
                "HyperSpy don't prune the file when saving it in "
                "Velox."
            )
            return

        def _read_stream(key):
            stream = FeiSpectrumStream(spectrum_stream_group[key], self)
            return stream

        subgroup_keys = _get_keys_from_group(spectrum_stream_group)
        if self.sum_EDS_detectors:
            if len(subgroup_keys) == 1:
                _logger.warning("The file contains only one spectrum stream")
            # Read the first stream
            s0 = _read_stream(subgroup_keys[0])
            streams = [s0]
            # add other stream streams
            if len(subgroup_keys) > 1:
                for key in subgroup_keys[1:]:
                    stream_data = spectrum_stream_group[key]["Data"][:].T[0]
                    if self.lazy:
                        s0.spectrum_image = (
                            s0.spectrum_image
                            + s0.stream_to_sparse_array(stream_data=stream_data)
                        )
                    else:
                        s0.stream_to_array(
                            stream_data=stream_data, spectrum_image=s0.spectrum_image
                        )
        else:
            streams = [_read_stream(key) for key in subgroup_keys]
        if self.lazy:
            for stream in streams:
                sa = stream.spectrum_image.astype(self.SI_data_dtype)
                stream.spectrum_image = sa

        spectrum_image_shape = streams[0].shape
        original_metadata = streams[0].original_metadata
        original_metadata.update(self.original_metadata)

        # Can be used in more recent version of velox emd files
        self.detector_information = self._get_detector_information(original_metadata)

        pixel_size, offsets, original_units = streams[0].get_pixelsize_offset_unit()
        dispersion, offset, unit = self._get_dispersion_offset(original_metadata)

        scale_x = self._convert_scale_units(
            pixel_size["width"], original_units, spectrum_image_shape[1]
        )
        scale_y = self._convert_scale_units(
            pixel_size["height"], original_units, spectrum_image_shape[0]
        )
        offset_x = self._convert_scale_units(
            offsets["x"], original_units, spectrum_image_shape[1]
        )
        offset_y = self._convert_scale_units(
            offsets["y"], original_units, spectrum_image_shape[0]
        )

        i = 0
        axes = []
        # add a supplementary axes when we import all frames individualy
        if not self.sum_frames:
            frame_time, time_unit = self._parse_frame_time(
                original_metadata, spectrum_image_shape[i]
            )
            axes.append(
                {
                    "index_in_array": i,
                    "name": "Time",
                    "offset": 0,
                    "scale": frame_time,
                    "size": spectrum_image_shape[i],
                    "units": time_unit,
                    "navigate": True,
                }
            )
            i = 1
        axes.extend(
            [
                {
                    "index_in_array": i,
                    "name": "y",
                    "offset": offset_y[0],
                    "scale": scale_y[0],
                    "size": spectrum_image_shape[i],
                    "units": scale_y[1],
                    "navigate": True,
                },
                {
                    "index_in_array": i + 1,
                    "name": "x",
                    "offset": offset_x[0],
                    "scale": scale_x[0],
                    "size": spectrum_image_shape[i + 1],
                    "units": scale_x[1],
                    "navigate": True,
                },
                {
                    "index_in_array": i + 2,
                    "name": "X-ray energy",
                    "offset": offset,
                    "scale": dispersion,
                    "size": spectrum_image_shape[i + 2],
                    "units": unit,
                    "navigate": False,
                },
            ]
        )

        md = self._get_metadata_dict(original_metadata)
        md["Signal"]["signal_type"] = "EDS_TEM"

        for stream in streams:
            original_metadata = stream.original_metadata
            original_metadata.update(self.original_metadata)
            self.dictionaries.append(
                {
                    "data": stream.spectrum_image,
                    "axes": axes,
                    "metadata": md,
                    "original_metadata": original_metadata,
                    "mapping": self._get_mapping(
                        parse_individual_EDS_detector_metadata=not self.sum_frames
                    ),
                }
            )

    def _get_dispersion_offset(self, original_metadata):
        try:
            for detectorname, detector in original_metadata["Detectors"].items():
                if (
                    original_metadata["BinaryResult"]["Detector"]
                    in detector["DetectorName"]
                ):
                    dispersion = (
                        float(detector["Dispersion"]) / 1000.0 * self.rebin_energy
                    )
                    offset = float(detector["OffsetEnergy"]) / 1000.0
                    return dispersion, offset, "keV"
        except KeyError:
            _logger.warning("The spectrum calibration can't be loaded.")
            return 1, 0, None

    def _convert_scale_units(self, value, units, factor=1):
        if units is None:
            return value, units
        factor /= 2
        v = float(value) * _UREG(units)
        converted_v = (factor * v).to_compact()
        converted_value = float(converted_v.magnitude / factor)
        converted_units = "{:~}".format(converted_v.units)
        return converted_value, converted_units

    def _get_metadata_dict(self, om):
        meta_gen = {}
        meta_gen["original_filename"] = os.path.split(self.filename)[1]
        if self.detector_name is not None:
            meta_gen["title"] = self.detector_name
        # We have only one entry in the original_metadata, so we can't use
        # the mapping of the original_metadata to set the date and time in
        # the metadata: need to set it manually here
        try:
            if "AcquisitionStartDatetime" in om["Acquisition"].keys():
                unix_time = om["Acquisition"]["AcquisitionStartDatetime"]["DateTime"]
            # Workaround when the 'AcquisitionStartDatetime' key is missing
            # This timestamp corresponds to when the data is stored
            elif (
                not isinstance(om["CustomProperties"], str)
                and "Detectors[BM-Ceta].TimeStamp" in om["CustomProperties"].keys()
            ):
                unix_time = (
                    float(
                        om["CustomProperties"]["Detectors[BM-Ceta].TimeStamp"]["value"]
                    )
                    / 1e6
                )
            date, time = self._convert_datetime(unix_time).split("T")
            meta_gen["date"] = date
            meta_gen["time"] = time
            meta_gen["time_zone"] = self._get_local_time_zone()
        except (UnboundLocalError):
            pass

        meta_sig = {}
        meta_sig["signal_type"] = ""

        return {"General": meta_gen, "Signal": meta_sig}

    def _get_mapping(
        self, map_selected_element=True, parse_individual_EDS_detector_metadata=True
    ):
        mapping = {
            "Acquisition.AcquisitionStartDatetime.DateTime": (
                "General.time_zone",
                lambda x: self._get_local_time_zone(),
            ),
            "Optics.AccelerationVoltage": (
                "Acquisition_instrument.TEM.beam_energy",
                lambda x: float(x) / 1e3,
            ),
            "Optics.CameraLength": (
                "Acquisition_instrument.TEM.camera_length",
                lambda x: float(x) * 1e3,
            ),
            "CustomProperties.StemMagnification.value": (
                "Acquisition_instrument.TEM.magnification",
                lambda x: float(x),
            ),
            "Instrument.InstrumentClass": (
                "Acquisition_instrument.TEM.microscope",
                None,
            ),
            "Stage.AlphaTilt": (
                "Acquisition_instrument.TEM.Stage.tilt_alpha",
                lambda x: round(np.degrees(float(x)), 3),
            ),
            "Stage.BetaTilt": (
                "Acquisition_instrument.TEM.Stage.tilt_beta",
                lambda x: round(np.degrees(float(x)), 3),
            ),
            "Stage.Position.x": (
                "Acquisition_instrument.TEM.Stage.x",
                lambda x: round(float(x), 6),
            ),
            "Stage.Position.y": (
                "Acquisition_instrument.TEM.Stage.y",
                lambda x: round(float(x), 6),
            ),
            "Stage.Position.z": (
                "Acquisition_instrument.TEM.Stage.z",
                lambda x: round(float(x), 6),
            ),
            "ImportedDataParameter.Number_of_frames": (
                "Acquisition_instrument.TEM.Detector.EDS.number_of_frames",
                None,
            ),
            "DetectorMetadata.ElevationAngle": (
                "Acquisition_instrument.TEM.Detector.EDS.elevation_angle",
                lambda x: round(float(x), 3),
            ),
            "DetectorMetadata.Gain": (
                "Signal.Noise_properties.Variance_linear_model.gain_factor",
                lambda x: float(x),
            ),
            "DetectorMetadata.Offset": (
                "Signal.Noise_properties.Variance_linear_model.gain_offset",
                lambda x: float(x),
            ),
        }

        # Parse individual metadata for each EDS detector
        if parse_individual_EDS_detector_metadata:
            mapping.update(
                {
                    "DetectorMetadata.AzimuthAngle": (
                        "Acquisition_instrument.TEM.Detector.EDS.azimuth_angle",
                        lambda x: "{:.3f}".format(np.degrees(float(x))),
                    ),
                    "DetectorMetadata.LiveTime": (
                        "Acquisition_instrument.TEM.Detector.EDS.live_time",
                        lambda x: "{:.6f}".format(float(x)),
                    ),
                    "DetectorMetadata.RealTime": (
                        "Acquisition_instrument.TEM.Detector.EDS.real_time",
                        lambda x: "{:.6f}".format(float(x)),
                    ),
                    "DetectorMetadata.DetectorName": ("General.title", None),
                }
            )

        # Add selected element
        if map_selected_element:
            mapping.update(
                {
                    "Operations.ImageQuantificationOperation": (
                        "Sample.elements",
                        self._convert_element_list,
                    ),
                }
            )

        return mapping

    def _convert_element_list(self, d):
        atomic_number_list = d[d.keys()[0]]["elementSelection"]
        return [
            atomic_number2name[int(atomic_number)]
            for atomic_number in atomic_number_list
        ]

    def _convert_datetime(self, unix_time):
        # Since we don't know the actual time zone of where the data have been
        # acquired, we convert the datetime to the local time for convenience
        dt = datetime.fromtimestamp(float(unix_time), tz=tz.tzutc())
        return dt.astimezone(tz.tzlocal()).isoformat().split("+")[0]

    def _get_local_time_zone(self):
        return tz.tzlocal().tzname(datetime.today())


# Below some information we have got from FEI about the format of the stream:
#
# The SI data is stored as a spectrum stream, ‘65535’ means next pixel
# (these markers are also called `Gate pulse`), other numbers mean a spectrum
# count in that bin for that pixel.
# For the size of the spectrum image and dispersion you have to look in
# AcquisitionSettings.
# The spectrum image cube itself stored in a compressed format, that is
# not easy to decode.


class FeiSpectrumStream(object):
    """Read spectrum image stored in FEI's stream format

    Once initialized, the instance of this class supports numpy style
    indexing and slicing of the data stored in the stream format.
    """

    def __init__(self, stream_group, reader):
        self.reader = reader
        self.stream_group = stream_group
        # Parse acquisition settings to get bin_count and dtype
        acquisition_settings_group = stream_group["AcquisitionSettings"]
        acquisition_settings = json.loads(acquisition_settings_group[0].decode("utf-8"))
        self.bin_count = int(acquisition_settings["bincount"])
        if self.bin_count % self.reader.rebin_energy != 0:
            raise ValueError(
                "The `rebin_energy` needs to be a divisor of the",
                " total number of channels.",
            )
        if self.reader.SI_data_dtype is None:
            self.reader.SI_data_dtype = acquisition_settings["StreamEncoding"]
        # Parse the rest of the metadata for storage
        self.original_metadata = _parse_sub_data_group_metadata(stream_group)
        # If last_frame is None, compute it
        stream_data = self.stream_group["Data"][:].T[0]
        if self.reader.last_frame is None:
            # The information could not be retrieved from metadata
            # we compute, which involves iterating once over the whole stream.
            # This is required to support the `last_frame` feature without
            # duplicating the functions as currently numba does not support
            # parametetrization.
            spatial_shape = self.reader.spatial_shape
            last_frame = int(
                np.ceil(
                    (stream_data == 65535).sum() / (spatial_shape[0] * spatial_shape[1])
                )
            )
            self.reader.last_frame = last_frame
            self.reader.number_of_frames = last_frame
        self.original_metadata["ImportedDataParameter"] = {
            "First_frame": self.reader.first_frame,
            "Last_frame": self.reader.last_frame,
            "Number_of_frames": self.reader.number_of_frames,
            "Rebin_energy": self.reader.rebin_energy,
            "Number_of_channels": self.bin_count,
        }
        # Convert stream to spectrum image
        if self.reader.lazy:
            self.spectrum_image = self.stream_to_sparse_array(stream_data=stream_data)
        else:
            self.spectrum_image = self.stream_to_array(stream_data=stream_data)

    @property
    def shape(self):
        return self.spectrum_image.shape

    def get_pixelsize_offset_unit(self):
        om_br = self.original_metadata["BinaryResult"]
        return om_br["PixelSize"], om_br["Offset"], om_br["PixelUnitX"]

    def stream_to_sparse_array(self, stream_data):
        """Convert stream in sparse array

        Parameters
        ----------
        stream_data: array

        """
        # Here we load the stream data into memory, which is fine is the
        # arrays are small. We could load them lazily when lazy.
        stream_data = self.stream_group["Data"][:].T[0]
        sparse_array = stream_readers.stream_to_sparse_COO_array(
            stream_data=stream_data,
            spatial_shape=self.reader.spatial_shape,
            first_frame=self.reader.first_frame,
            last_frame=self.reader.last_frame,
            channels=self.bin_count,
            sum_frames=self.reader.sum_frames,
            rebin_energy=self.reader.rebin_energy,
        )
        return sparse_array

    def stream_to_array(self, stream_data, spectrum_image=None):
        """Convert stream to array.

        Parameters
        ----------
        stream_data: array
        spectrum_image: array or None
            If array, the data from the stream are added to the array.
            Otherwise it creates a new array and returns it.

        """
        spectrum_image = stream_readers.stream_to_array(
            stream=stream_data,
            spatial_shape=self.reader.spatial_shape,
            channels=self.bin_count,
            first_frame=self.reader.first_frame,
            last_frame=self.reader.last_frame,
            rebin_energy=self.reader.rebin_energy,
            sum_frames=self.reader.sum_frames,
            spectrum_image=spectrum_image,
            dtype=self.reader.SI_data_dtype,
        )
        return spectrum_image


def read_emd_version(group):
    """Function to read the emd file version from a group. The EMD version is
    saved in the attributes 'version_major' and 'version_minor'.

    Parameters
    ----------
    group : hdf5 group
        The group to extract the version from.

    Returns
    -------
    file version : str
        Empty string if the file version is not defined in this group

    """
    major = group.attrs.get("version_major", None)
    minor = group.attrs.get("version_minor", None)
    if major is not None and minor is not None:
        return f"{major}.{minor}"
    else:
        return ""


def is_EMD_NCEM(file):
    """
    Parameters
    ----------
    file : h5py file handle
        DESCRIPTION.

    Returns
    -------
    bool
        DESCRIPTION.

    """

    def _is_EMD_NCEM(file):
        for group in file:
            if read_emd_version != "":
                return True
        return False

    if isinstance(file, str):
        with h5py.File(file, "r") as f:
            return _is_EMD_NCEM(f)
    else:
        return _is_EMD_NCEM(file)


def is_EMD_Velox(file):
    """Function to check if the EMD file is an Velox file.

    Parameters
    ----------
    file : string or HDF5 file handle
        The name of the emd-file from which to load the signals. Standard
        file extension is 'emd'.

    Returns
    -------
    True if the file is a Velox file, otherwise False

    """

    def _is_EMD_velox(file):
        if "Version" in list(file.keys()):
            version = file.get("Version")
            v_dict = json.loads(version[0].decode("utf-8"))
            if v_dict["format"] in ["Velox", "DevelopersKit"]:
                return True
        return False

    if isinstance(file, str):
        with h5py.File(file, "r") as f:
            return _is_EMD_velox(f)
    else:
        return _is_EMD_velox(file)


def file_reader(filename, lazy=False, **kwds):
    """
    Read EMD file, which can be a NCEM or a Velox variant of the EMD format.
    Also reads Direct Electron's DE5 format, which is read as if it is the
    NCEM format.

    Parameters
    ----------
    filename : str
        Filename of the file to write.
    lazy : bool
        Open the data lazily. Default is False.
    **kwds : dict
        Keyword argument pass to the EMD NCEM or EMD Velox reader. See user
        guide or the docstring of the `load` function for more information.
    """
    file = h5py.File(filename, "r")
    dictionaries = []
    try:
        if is_EMD_Velox(file):
            _logger.debug("EMD file is a Velox variant.")
            emd_reader = FeiEMDReader(lazy=lazy, **kwds)
            emd_reader.read_file(file)
        elif is_EMD_NCEM(file):
            _logger.debug("EMD file is a Berkeley variant.")
            dataset_path = kwds.pop("dataset_path", None)
            stack_group = kwds.pop("stack_group", None)
            emd_reader = EMD_NCEM(**kwds)
            emd_reader.read_file(
                file, lazy=lazy, dataset_path=dataset_path, stack_group=stack_group
            )
        else:
            raise IOError("The file is not a supported EMD file.")
    except Exception as e:
        raise e
    finally:
        if not lazy:
            file.close()

    dictionaries = emd_reader.dictionaries

    return dictionaries


def file_writer(filename, signal, **kwds):
    """
    Write signal to EMD NCEM file.

    Parameters
    ----------
    file : str of h5py file handle
        If str, filename of the file to write, otherwise a h5py file handle
    signal : instance of hyperspy signal
        The signal to save.
    **kwargs : dict
        Dictionary containing metadata which will be written as attribute
        of the root group.
    """
    EMD_NCEM().write_file(filename, signal, **kwds)
